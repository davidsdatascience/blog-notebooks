{
 "metadata": {
  "name": "",
  "signature": "sha256:b3996e3b78824f0fada01f4dbdcc4d921ee85b8fbfc36253091079ac3212a09e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Recently I've been playing a lot with MCMC sampling and in particular the Metropolis-Hastings algorithm. MH requires a proposal distribution that can generate subsequent samples for the next state in the markov chain, eventually filling out the posterior distribution. \n",
      "\n",
      "Typically a Gaussian distribution centred around the currently value is used as the proposal distribution. However, sometimes the support of the target distribution isn't the entire real line (e.g. the mean of a negative binomial distribution must be positive). In this case, sampling from a truncated normal distribution with lower bound 0 makes a lot of sense. Coding in python, I used the `truncnorm` class from `scipy.stats` to sample:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats import truncnorm\n",
      "import numpy as np\n",
      "\n",
      "np.random.seed(123)\n",
      "\n",
      "lower_clip = 0.\n",
      "upper_clip = np.inf\n",
      "mu = 3.\n",
      "sd = 2.\n",
      "\n",
      "truncnorm.rvs((lower_clip - mu) / sd, (upper_clip - mu) / sd, mu, sd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "4.1464110604259821"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So far so good. However, `truncnorm.rvs` isn't vectorised for different `mu`, which comes in really handy in regression settings such as negative binomial regression. \n",
      "\n",
      "No matter, we can write a wrapper function and use `np.vectorize`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def truncnorm_rvs_wrapper(mu, sd, lower_clip, upper_clip):\n",
      "    return truncnorm.rvs((lower_clip - mu) / sd, upper_clip, mu, sd)\n",
      "    \n",
      "truncnorm_rvs = np.vectorize(truncnorm_rvs_wrapper, otypes=[np.float])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can sample for a whole range of `mu`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mus = np.random.randint(1,10,50)\n",
      "truncnorm_rvs(mus, sd, lower_clip, upper_clip)[1:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "array([ 1.95424841,  6.9155242 ,  6.50475593,  4.15263574,  7.57421282,\n",
        "        0.71420649,  3.34918575,  2.85799086,  1.96588436])"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "However, compared to sampling from the usual Gaussian distribution, this is slow. Like, _really_ slow:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit truncnorm_rvs(mus, sd, lower_clip, upper_clip)\n",
      "%timeit np.random.normal(mus, sd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "100 loops, best of 3: 1.27 ms per loop\n",
        "100000 loops, best of 3: 10.9 \u00b5s per loop"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While 1.27ms doesn't seem like much, when this gets evaluated thousands of times per sample it can really start to hold things up and is almost two orders of magnitude slower than sampling from a normal distribution. So the question becomes, can we implement truncated normal sampling that is almost as fast as normal normal sampling?\n",
      "\n",
      "Consider the following function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def truncnorm_rvs_recursive(x, sigma, lower_clip):\n",
      "    q = np.random.normal(x, sigma, size=len(x))\n",
      "    if np.any(q < lower_clip):\n",
      "        q[q < lower_clip] = truncnorm_rvs_recursive(x[q < lower_clip], sigma, lower_clip)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%timeit truncnorm_rvs_recursive(mus, sd, lower_clip)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10000 loops, best of 3: 52.8 \u00b5s per loop\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And just like that, we've improved on the stock `truncnorm.rvs` by a factor of around 50. While we haven't implemented an upper bound, it could easily be extended to such a scenario (though would probably trigger further recursions).\n",
      "\n",
      "One of the problems with such an approach is that the number of recursions is itself probabilistic and data-dependent - the number of recursions will depend on the number of samples generated each time that lie outside `lower_clip`. Can we put a bound on the expectation of the number of recursions? We know sampling from the standard normal is around 100 time faster than `truncnorm.rvs`, so as long as the number of recursions is less than 100 our method is still faster.\n",
      "\n",
      "For the distribution with only a `lower_clip`, the probability a sample will be generated outside the plausible range is given by \\\\(Pr(\\mathrm{outside}) = \\Phi(L)\\\\) for a lower bound \\\\(L\\\\) where \\\\(\\Phi\\\\) is the cumulative normal distribution. \n",
      "\n",
      "Say we take the rather extreme situation where the means of the truncated distribution are all the same as `lower_clip`. In this case, the probability of a sample being rejected is 0.5 and so the expected number of elements that will fall outside the allowed range is \\\\(N/2\\\\) for the mean vector of length \\\\(N\\\\). Then the expected number of elements falling outside on the \\\\(i^{th}\\\\) recursion is given by \\\\( \\frac{N}{2^i} \\\\). We expect this function to stop recursing at recursion \\\\(m\\\\) when \\\\(\\frac{N}{2^m} < 0.5\\\\) (this is of course approximate since we could continue recursing for a further \\\\(p\\\\) recursions with probability \\\\(0.5^p\\\\)). Solving for \\\\(m\\\\) we get\n",
      "\n",
      "$$ m = log_2 N + 1 $$\n",
      "\n",
      "In other words, the _expected_ number of recursions scales logarithmically in \\\\(N\\\\), making it particularly promising as a competitor (at least for lower bounded) truncated normal sampling in `python`."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}